{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6e0fa4-8b73-4fed-b96a-f808a677fc85",
   "metadata": {},
   "source": [
    "\n",
    "# Model Evaluation (scikit-learn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f75c2-697f-4233-8beb-87c229df759c",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction].(#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2e325-c05c-4967-ac7a-d88b3486f0b1",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace33be-0f27-423c-8795-99a38135f723",
   "metadata": {},
   "source": [
    "This is a file with functions that aim to facilitate the performance analysis of models. As output, the functions provide graphs, metrics, and basic tables commonly used in the analysis process. Pay attention to the functions parameters:\n",
    "\n",
    "- **final_model**: developed model\n",
    "    > Save as **final_model_*type***\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- **model**: type of developed model:\n",
    "    - **lgbm**: Light Gradient Boosting Machine;\n",
    "    - **loglin**: Linear Regression;\n",
    "    - **logreg**: Logistic Regression;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- **X**: test or validation database;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- **y**: test or validation target;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa4e2f-65f7-4d73-a816-26d3ff3666c3",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0fcf3-f148-419c-9e8f-9879773cd5b1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import of packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31c0482-1102-49ba-863b-e8643bea247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068f73ac-9f22-4577-97ea-edaa55ab7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # parquet\n",
    "import pickle # pickle\n",
    "\n",
    "# interative interface\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732dda2-60b2-46b9-9c59-faaf804fa5d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 2. Import of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401be881-d5dd-49a3-b675-f1aaeb836381",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcf4721-661a-43d7-8c1c-61ca227d4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('X_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f814040-12a3-4e55-8c80-3c5f535ecd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet('y_train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4460046-a245-4da9-81e5-6e4ab25b7901",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aae338d-9ea2-4788-b6cc-7e9038942f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet('X_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b07a99-c01c-4e98-a97b-c2e2c82ba00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_parquet('y_test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891df6d-a124-4f47-bc5e-2fc5698c83bc",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8eb5764-1dc9-49b5-b05e-e03081094547",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = pd.read_parquet('X_validation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d4e68-5954-4b7c-9ade-bed3f748444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = pd.read_parquet('y_validation.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6291b-8b24-4db8-91f5-7d3e55ae9797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 3. Import of trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5808cc4-bf92-4e76-83bf-e293e13ba899",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_model_type.pkl', 'rb') as trained_model:\n",
    "    model = pickle.load(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30edad-52c0-4e0a-ba90-7a37862d20c1",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee28c90-996b-40bf-89b2-b68308bc7bab",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Fuctions definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbec46-069c-4b13-8d6d-b2f101336e6d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1. Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641e707-37ca-479c-91ee-c35122fbe3d9",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.1. Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86028e10-3d67-45a8-b79f-a688a0697fc6",
   "metadata": {},
   "source": [
    "**Accuracy (accuracy_score)**:\n",
    "\n",
    "- Definition: Accuracy measures the proportion of correct predictions out of the total predictions made. It is a common metric for classification problems but can be misleading when classes are imbalanced.\n",
    "- Note: Accuracy does not take class imbalances into account, so it might not be the best choice when classes have vastly different sizes.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Balanced Accuracy (balanced_accuracy_score)**:\n",
    "\n",
    "- Definition: Balanced accuracy takes class imbalance into account by calculating the accuracy of labels for each class and then averaging them, weighted by the proportion of samples in each class.\n",
    "- Note: It is especially useful when there is class imbalance, providing a more accurate measure of model performance in such cases.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Top-k Accuracy (top_k_accuracy_score)**:\n",
    "\n",
    "- Definition: Top-k accuracy considers a prediction correct if the true class is among the top k predictions made by the model.\n",
    "- Note: It is useful when you are interested in knowing if the model can predict the true class among the top k predictions, rather than just the top prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d507b3d3-e5da-4c0d-b2e7-ce7da7085b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ffb5e-299f-47ae-9b62-845466dc3422",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.2. Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf59e91-e1d4-4194-807c-a912cce56152",
   "metadata": {},
   "source": [
    "**Average Precision (average_precision_score)**:\n",
    "\n",
    "- Definition: Average Precision is calculated as the average of precision for each possible classification threshold. In other words, it is the area under the precision-recall curve (area under the PR curve).\n",
    "- Note: It is useful when you are interested in the precision of positive predictions, taking into account the true positive rate and false positive rate. This metric is especially important when there is class imbalance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Precision (precision_score)**:\n",
    "\n",
    "- Definition: Precision is the ratio of true positives (samples correctly predicted as positive) to the total positive predictions (true positives + false positives).\n",
    "- Note: Precision is a valuable metric when the focus is on minimizing false positives. It is particularly important in situations where false positives are costly or problematic.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In summary, average_precision is a more comprehensive metric that takes into account multiple classification thresholds and is useful for imbalanced classification problems, whereas precision is a specific metric that focuses on the precision of positive predictions relative to the total positive predictions made by the model. The choice between these metrics depends on the specific goals of your classification problem and the relative importance of false positives compared to other types of classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8e509a-36d3-4951-be09-adae23330193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7072e7f-8658-438f-8a5f-996d0208ec67",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.3. neg_brier_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6def0-cf25-4e07-9fc5-d76136ea1eec",
   "metadata": {},
   "source": [
    "**neg_brier_score**:\n",
    "\n",
    "- Definition: \n",
    "- Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d2e3ae-ad15-4db2-8e46-3826b8d70597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406553b-4557-4d73-96e4-2fedf2c3be3d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.4. F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c229d7-aa20-4669-ba4c-2d0863d93317",
   "metadata": {},
   "source": [
    "**F1 Score (f1_score)**:\n",
    "\n",
    "- Definition: F1 Score is the harmonic mean of precision and recall. It is useful when there is an imbalance between classes in the dataset.\n",
    "- Note: It is an overall F1 metric that gives equal weight to precision and recall.\n",
    "\n",
    "**F1 Micro-Averaged Score (f1_micro)**:\n",
    "\n",
    "- Definition: Calculates the overall F1 score by aggregating the counts of true positives, false positives, and false negatives, and then computing precision and recall based on these sums.\n",
    "- Note: Useful when you want to calculate F1 metric for multiclass classification problems by aggregating total counts of true positives, false positives, and false negatives.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**F1 Macro-Averaged Score (f1_macro)**:\n",
    "\n",
    "- Definition: Calculates the F1 score for each class individually and then takes the average of these scores to obtain the macro-F1 score.\n",
    "- Note: Useful when you want to treat each class with equal importance regardless of the number of samples in each class.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**F1 Weighted-Averaged Score (f1_weighted)**:\n",
    "\n",
    "- Definition: Calculates the F1 score for each class and then takes the average of these scores, weighted by the number of samples in each class.\n",
    "- Note: Useful when you want to treat each class with importance proportional to the number of samples in the class.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**F1 Samples-Averaged Score (f1_samples)**:\n",
    "\n",
    "- Definition: Calculates the F1 score for each sample individually and then takes the average of these scores to obtain the F1 score per sample.\n",
    "- Note: Useful when you want to calculate the F1 metric for multilabel classification problems.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In summary, the choice between f1_micro, f1_macro, f1_weighted, and f1_samples depends on your specific problem and evaluation needs, while f1_score is used when you want the F1 score without specifying a specific averaging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef51912-7272-4976-9b61-65e02d74f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa971e1-6714-4656-8e93-aa142660c81b",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.5. neg_log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aceba6-3561-4cd1-846b-ba823f9c2ffd",
   "metadata": {},
   "source": [
    "**neg_brier_score**:\n",
    "\n",
    "- Definition: \n",
    "- Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af8c3a2-2ffb-4818-a17d-d9daf2a53008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588664d7-a172-4c36-896e-1b1bb5f226d8",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.6. Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530ad06-5a11-4aa0-b04a-bba8000fcbb5",
   "metadata": {},
   "source": [
    "**recall (recall_score)**:\n",
    "\n",
    "- Definition: \n",
    "- Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90137aaf-e6e9-4a04-93c9-257ebb8ce020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9746ef9-3154-466c-ac9c-371458d3158d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.7. jaccard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e5dbf-8689-4ec8-89d0-32ac7e46aa06",
   "metadata": {},
   "source": [
    "**jaccard (jaccard_score)**:\n",
    "\n",
    "- Definition: \n",
    "- Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bafa3afc-151a-4a3b-8452-b5a123d66869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae4221-2dd2-45bf-bb49-ea8c75b6d569",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1.8. ROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516453a5-5028-44ec-91e9-f4716ce4779f",
   "metadata": {},
   "source": [
    "**ROC AUC (roc_auc_score)**:\n",
    "\n",
    "- Definition: Computes the area under the ROC curve for binary classification problems. The ROC curve shows the true positive rate versus false positive rate for different classification thresholds.\n",
    "- Note: It is a general metric for evaluating binary classifiers. The higher the value, the better the model's performance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**ROC AUC One-Versus-Rest (roc_auc_ovr)**:\n",
    "\n",
    "- Definition: Computes the area under the ROC curve for multiclass classification problems using the one-versus-rest strategy. Each class is treated as the positive class, while the others are grouped as the negative class.\n",
    "- Note: Useful when you have a multiclass classification problem.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**ROC AUC One-Versus-One (roc_auc_ovo)**:\n",
    "\n",
    "- Definition: Computes the area under the ROC curve for multiclass classification problems using the one-versus-one strategy. Each pair of classes is treated separately, and the average of the ROC AUC values is calculated.\n",
    "- Note: Useful when you have a multiclass classification problem, especially with many classes, and one-versus-rest strategy would result in many comparisons.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**ROC AUC One-Versus-Rest Weighted (roc_auc_ovr_weighted)**:\n",
    "\n",
    "- Definition: Computes the area under the ROC curve for multiclass classification problems using the one-versus-rest strategy, with classes weighted by the number of samples in each class.\n",
    "- Note: Weights the metric by the number of samples in each class, useful when there is class imbalance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**ROC AUC One-Versus-One Weighted (roc_auc_ovo_weighted)**:\n",
    "\n",
    "- Definition: Computes the area under the ROC curve for multiclass classification problems using the one-versus-one strategy, with classes weighted by the number of samples in each class.\n",
    "- Note: Weights the metric by the number of samples in each class, useful when there is class imbalance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In summary, the choice between roc_auc, roc_auc_ovr, roc_auc_ovo, roc_auc_ovr_weighted, and roc_auc_ovo_weighted depends on the type of classification problem you are dealing with (binary or multiclass) and specific evaluation needs, including handling class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eed61bea-78f6-4242-a7b5-408174c452ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab15f1-9d41-41b0-9a17-7f0e592cd125",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### 4.2. Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575b40e-787a-475a-bbf1-4fcc69b42d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### 4.2.1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ddb63-ee48-44da-b4ac-368253aa27ce",
   "metadata": {},
   "source": [
    "** **:\n",
    "\n",
    "- Definition: \n",
    "- Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef646ad-531d-4c74-8243-68872b936b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe194fb1-7836-4d06-bb27-ecf41f956cf8",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2. Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023ffd0-49a6-4de3-82e0-9dea5ee4123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_graphs(model, final_model, y, X):\n",
    "    # Implement your basic_graphs function here\n",
    "    print(\"Calling basic_graphs with parameters:\")\n",
    "    print(\"Model:\", model)\n",
    "    print(\"Final Model:\", final_model)\n",
    "    print(\"y:\", y)\n",
    "    print(\"X:\", X)\n",
    "\n",
    "def basic_tables(model, final_model, y, X):\n",
    "    # Implement your basic_tables function here\n",
    "    print(\"Calling basic_tables with parameters:\")\n",
    "    print(\"Model:\", model)\n",
    "    print(\"Final Model:\", final_model)\n",
    "    print(\"y:\", y)\n",
    "    print(\"X:\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35eff57-5b1b-4ddd-bc9b-e1ac860a4d59",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba840a4c-84f6-4c33-9267-b4c6b506d855",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Interactive interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9509591-9255-4a69-90df-26b596b6922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(Deprecated) Uninstalling extensions with the jupyter labextension uninstall command is now deprecated and will be removed in a future major version of JupyterLab.\n",
      "\n",
      "Users should manage prebuilt extensions with package managers like pip and conda, and extension authors are encouraged to distribute their extensions as prebuilt packages \u001b[0m\n",
      "An error occurred.\n",
      "ValueError: Please install Node.js and npm before continuing installation. You may be able to install Node.js from your package manager, from conda, or directly from the Node.js website (https://nodejs.org).\n",
      "See the log file for details:  /tmp/jupyterlab-debug-ub5ng281.log\n"
     ]
    }
   ],
   "source": [
    "!jupyter labextension uninstall jupyter-matplotlib && jupyter labextension uninstall @jupyter-widgets/jupyterlab-manager && conda update -y widgetsnbextension && conda update -y nodejs && pip uninstall -y ipympl && pip install git+https://github.com/matplotlib/jupyter-matplotlib.git#egg=ipympl && conda update jupyterlab -y && jupyter labextension install @jupyter-widgets/jupyterlab-manager && jupyter labextension install jupyter-matplotlib && jupyter labextension update --all && jupyter lab build && jupyter nbextension list && jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70bdb154-593b-4224-aa09-96edad87ab07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cdfce5e5954750a3462b32c71e19b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='My Button', style=ButtonStyle())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# creating button\n",
    "widgets.Button(description = 'My Button')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "323bccda-d641-4c1f-98d3-5dacf5146da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b4acf751184e3a9e71a8534e03f88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lang()>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "\n",
    "from random import choice\n",
    "\n",
    "def lang():\n",
    "    langSelect = [\"English\",\"Deustche\",\"Espanol\",\"Italiano\",\"한국어\",\"日本人\"]\n",
    "    print(choice(langSelect))\n",
    "\n",
    "interact_manual(lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f0111-385e-4aa6-b1bd-96a76e46a5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f241460e-effa-4139-b2bf-78f716a22e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8a7b53d31f49f2bbf09387d302d7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Model:'), Text(value='', description='Final Model:'), Text(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e3041f17b64a048029b5de7396a5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "\n",
    "# Create input widgets\n",
    "model_widget = widgets.Text(description='Model:')\n",
    "final_model_widget = widgets.Text(description='Final Model:')\n",
    "y_widget = widgets.Text(description='y:')\n",
    "X_widget = widgets.Text(description='X:')\n",
    "\n",
    "function_dropdown = widgets.Dropdown(\n",
    "    options=['basic_graphs', 'basic_tables'],\n",
    "    description='Select Function:'\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description='Run')\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def run_button_click(b):\n",
    "    selected_function = function_dropdown.value\n",
    "    model = model_widget.value\n",
    "    final_model = final_model_widget.value\n",
    "    y = y_widget.value\n",
    "    X = X_widget.value\n",
    "    \n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"sagemaker_interface.py\",\n",
    "            \"--function\", selected_function,\n",
    "            \"--model\", model,\n",
    "            \"--final_model\", final_model,\n",
    "            \"--y\", y,\n",
    "            \"--X\", X],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "\n",
    "run_button.on_click(run_button_click)\n",
    "\n",
    "# Display widgets\n",
    "input_widgets = widgets.VBox([model_widget, final_model_widget, y_widget, X_widget, function_dropdown, run_button])\n",
    "display(input_widgets, output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bb205-f396-41ce-b01b-48097ca1b141",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dddca5-27f5-491f-826a-2bb9ec5512a4",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852b901-0cf8-4054-97e3-7e131f99e342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
